{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import ydata_profiling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select paths and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_type = \"tl123_edges\"\n",
    "snapshot = 1\n",
    "net_path = Path(f\"networks/{network_type}/{snapshot}_{network_type}.csv\")\n",
    "nodes_path = Path(f\"networks/{snapshot}_nodes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_path = Path(f\"analysis/{network_type}\")\n",
    "reports_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_df = pd.read_csv(nodes_path, index_col=0, sep=\",\")\n",
    "nodes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_edges = pd.read_csv(net_path, index_col=0)\n",
    "# _edges = _edges.rename(columns={\"lang_usage\": \"weight\"})  # uncomment for coursefiltered_edges\n",
    "_edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nx.from_pandas_edgelist(_edges, create_using=nx.DiGraph, edge_attr=\"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(net.edges), len(net.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_networkx(net, pos=nx.spring_layout(net))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centrality metrics computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_metrics = []\n",
    "\n",
    "_unweighted_net = copy.deepcopy(net)\n",
    "for (n1, n2, d) in _unweighted_net.edges(data=True):\n",
    "    d.clear()\n",
    "\n",
    "lst_metrics.append({\"in_degree_weighted\": dict(net.in_degree(weight=\"weight\"))})\n",
    "lst_metrics.append({\"in_degree\": dict(net.in_degree())})\n",
    "\n",
    "lst_metrics.append({\"out_degree_weighted\": dict(net.out_degree(weight=\"weight\"))})\n",
    "lst_metrics.append({\"out_degree\": dict(net.out_degree())})\n",
    "\n",
    "lst_metrics.append({\"degree_weighted\": dict(net.degree(weight=\"weight\"))})\n",
    "lst_metrics.append({\"degree\": dict(net.degree())})\n",
    "\n",
    "lst_metrics.append({\"betweenness_weighted\": nx.betweenness_centrality(net, weight=\"weight\")})\n",
    "lst_metrics.append({\"betweenness\": nx.betweenness_centrality(net)})\n",
    "\n",
    "lst_metrics.append({\"closeness\": nx.closeness_centrality(net)})\n",
    "\n",
    "lst_metrics.append({\"pagerank_weighhted\": nx.pagerank(net, weight=\"weight\")})\n",
    "\n",
    "lst_metrics.append({\"pagerank\": nx.pagerank(_unweighted_net)})\n",
    "\n",
    "lst_metrics.append({\"voterank\": {n: idx for idx, n in enumerate(nx.voterank(net))}})\n",
    "\n",
    "lst_metrics.append(\n",
    "    {\"clustering_coefficient\": {n: cc for n, cc in nx.clustering(net).items()}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metrics = {k:v for lm in lst_metrics for k, v in lm.items()}\n",
    "df_metrics = pd.DataFrame(dict_metrics)\n",
    "\n",
    "df_metrics.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of node attrs taken into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select metrics to keep and to normalise (metric name, a standarisation factor)\n",
    "improv_cols = {col: 5 for col in nodes_df.columns if \"improv\" in col}\n",
    "metric_cols = {\n",
    "    \"metric_postsojournOPI\": 11,\n",
    "    \"metric_presojournOPI\": 11,\n",
    "    \"metric_level gained\": 11,\n",
    "    \"metric_FLcumulativecompetence\": 1,\n",
    "    \"metric_general_cumulativecompet\": 1,\n",
    "    \"context_learningoutofclassminday\": 1440,\n",
    "    \"psycho_motivationdegree\": 5,\n",
    "    \"psycho_proficiencyingroup_BAL1\": 3,\n",
    "    \"interact_classmates\": 100,\n",
    "    \"interaction_groupintegration\": 5,\n",
    "    \"202_final\": 100,\n",
    "    # dummy variables to check if features that should be constant through all snapshots are in fact constant\n",
    "    \"living_sum_flatmates\": 1,\n",
    "    \"metric_general_cumulativecompet\": 1,\n",
    "    \"metric_youngersiblings\": 1,\n",
    "}\n",
    "investigated_node_attrs = pd.DataFrame()\n",
    "analysed_cols = {**metric_cols, **improv_cols}\n",
    "\n",
    "# add new metric\n",
    "investigated_node_attrs[\"metric_TLuseoutofclass\"] = (\n",
    "    nodes_df[\"psycho_TLuseoutofclassminday\"] / \n",
    "    (nodes_df[\"psycho_TLuseoutofclassminday\"] + nodes_df[\"psycho_otherlgsminday\"])\n",
    ")\n",
    "investigated_node_attrs = investigated_node_attrs.join(nodes_df[\"metric_Gender\"])\n",
    "\n",
    "# normalise metric_cols\n",
    "for col, norm_coef in analysed_cols.items():\n",
    "    # print(col, norm_coef)\n",
    "    investigated_node_attrs = investigated_node_attrs.join(\n",
    "        nodes_df[col].div(norm_coef, axis=0)\n",
    "    )\n",
    "\n",
    "# add two new metrics\n",
    "investigated_node_attrs[\"metric_presojournOPI_delta\"] = (\n",
    "    investigated_node_attrs[\"metric_presojournOPI\"] - investigated_node_attrs[\"202_final\"]\n",
    ")\n",
    "investigated_node_attrs[\"metric_postsojournOPI_delta\"] = (\n",
    "    investigated_node_attrs[\"metric_postsojournOPI\"] - investigated_node_attrs[\"202_final\"]\n",
    ")\n",
    "\n",
    "investigated_node_attrs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "investigated_node_attrs.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge both metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(investigated_node_attrs, df_metrics, left_index=True, right_index=True)\n",
    "merged_df.to_csv(reports_path / f\"{snapshot}_eda.csv\")\n",
    "merged_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = ydata_profiling.ProfileReport(\n",
    "    merged_df,\n",
    "    title=f\"EDA of snapshot {snapshot}\",\n",
    "    infer_dtypes=False,\n",
    "    explorative=True,\n",
    "    correlations={\n",
    "        \"auto\": {\"calculate\": True},\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": True},\n",
    "    },\n",
    ")\n",
    "report.to_file(reports_path / f\"{snapshot}_eda.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
