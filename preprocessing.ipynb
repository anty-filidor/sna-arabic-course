{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"networks\")\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "df_path = \"data/whitby_03_07_Peer Interaction in SA_Jordan_22_Questionnaire 1_2_3_total data set.csv\"\n",
    "snapshot = 3  # <- change this to obtain particulat snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(df_path) as f:\n",
    "    _cols = f.readline()[1:-2]\n",
    "_cols = _cols.split('\",\"')\n",
    "dtype_mapper = {c: str for c in _cols if (\"net@\" in c and \"choice\" not in c)}\n",
    "\n",
    "df = pd.read_csv(df_path, converters = dtype_mapper)\n",
    "df = df.drop(df.loc[df[\"group_questionnaire_order\"] != snapshot].index)\n",
    "df = df.set_index(\"metric_id\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_ego_cols = {col for col in df.columns if (\"net\" in col and \"choice\" in col)}\n",
    "edge_course_cols = {col for col in df.columns if \"net\" in col}.difference(edge_ego_cols)\n",
    "node_cols = set(df.columns).difference(edge_course_cols).difference(edge_ego_cols)\n",
    "\n",
    "assert len(edge_ego_cols) + len(edge_course_cols) + len(node_cols) == len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_ego_df = df[list(edge_ego_cols)]\n",
    "edge_course_df = df[list(edge_course_cols)]\n",
    "node_df = df[list(node_cols)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain weighted adjacency matrix for 'ego' network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_edge_list = []\n",
    "\n",
    "for node, row in edge_ego_df.iterrows():\n",
    "    for choice_level, choice_node in row.items():\n",
    "        try:\n",
    "            weight = int(choice_level[4])  # e.g. net@2ndchoice -> 2\n",
    "            choice_node = int(choice_node)\n",
    "            ego_edge_list.append({\"source\": node, \"target\": choice_node, \"weight\": weight})\n",
    "        except:\n",
    "            print(node, choice_node, choice_level)\n",
    "\n",
    "ego_edge_df = pd.DataFrame(ego_edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_edge_df[\"weight\"] =  ego_edge_df[\"weight\"].max() - ego_edge_df[\"weight\"] + 1\n",
    "ego_edge_df[\"weight\"] =  ego_edge_df[\"weight\"] / ego_edge_df[\"weight\"].max()\n",
    "ego_edge_df[\"weight\"].unique()\n",
    "# 1, 2, 3, 4, 5\n",
    "# 5, 4, 3, 2, 1 \n",
    "# 1, 0.8, 0.6, 0.4, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_edge_df.to_csv(out_path.joinpath(f\"{snapshot}_ego_edges.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain weighted edge list for course network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_edge(code: str) -> dict:\n",
    "\n",
    "    if len(code) != 3:\n",
    "        print(code)\n",
    "        return {}\n",
    "    try:\n",
    "        int(code)\n",
    "    except:\n",
    "        print(code)\n",
    "        return {}\n",
    "    \n",
    "    if code[0] == \"1\":\n",
    "        direction = \"out\"\n",
    "    elif code[0] == \"2\":\n",
    "        direction = \"in\"\n",
    "    elif code[0] == \"3\":\n",
    "        direction = \"mutual\"\n",
    "    else:\n",
    "        print(code)\n",
    "        return {}\n",
    "    intensity = int(code[1])  # not yet normalised! \n",
    "    lang_usage = int(code[2]) # not yet normalised!\n",
    "\n",
    "    return {\"direction\": direction, \"intensity\": intensity, \"lang_usage\": lang_usage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_edge_list = []\n",
    "\n",
    "for source_node, row in edge_course_df.iterrows():\n",
    "    for target_node, code in row.items():\n",
    "        decoded_edge = decode_edge(str(code))\n",
    "        if len(decoded_edge) == 0:\n",
    "            continue\n",
    "        course_edge_list.append({\"source\": source_node, \"target\": int(target_node[4:]), **decoded_edge})\n",
    "\n",
    "edge_course_df = pd.DataFrame(course_edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging - sanity chech whether decoded edges are in assumed ranges\n",
    "sns.histplot(edge_course_df[\"intensity\"])  # should be [0, 5]\n",
    "sns.mpl.pyplot.show()\n",
    "sns.histplot(edge_course_df[\"lang_usage\"])  # should be [0, 9]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise intensity and lang_usage\n",
    "edge_course_df[\"intensity\"] = edge_course_df[\"intensity\"].map(lambda x: max(0, min(5, x)))\n",
    "sns.histplot(edge_course_df[\"intensity\"])\n",
    "sns.mpl.pyplot.show()\n",
    "\n",
    "edge_course_df[\"lang_usage\"] = edge_course_df[\"lang_usage\"].map(lambda x: max(0, min(9, x)))\n",
    "sns.histplot(edge_course_df[\"lang_usage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_course_df.to_csv(out_path.joinpath(f\"{snapshot}_course_edges.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain node list - doesn't work yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that don't add any information, i.e. each node has the same value for these columns\n",
    "garbage_attrs = []\n",
    "for col in node_df.columns:\n",
    "    unique_vals = node_df[col].unique()\n",
    "    if len(unique_vals) == 1:\n",
    "        garbage_attrs.append(col)\n",
    "garbage_attrs.sort()\n",
    "\n",
    "# save names of renamed columns for further sainty check\n",
    "ga_path = out_path / \"trash\"\n",
    "ga_path.mkdir(exist_ok=True, parents=True)\n",
    "_ = {ga: str(node_df[ga].unique()[0]) for ga in garbage_attrs}\n",
    "pd.DataFrame({snapshot: _}).to_csv(ga_path / f\"{snapshot}_removed_columns.csv\")\n",
    "\n",
    "print(f\"removing {len(garbage_attrs)} columns\")\n",
    "node_df = node_df.drop(garbage_attrs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df = node_df.reset_index().rename({\"metric_id\": \"node_id\"}, axis=1)\n",
    "node_df.to_csv(out_path.joinpath(f\"{snapshot}_nodes.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse if sets of removed columns are the same\n",
    "\n",
    "Do it when all three snapshots are processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trashed_cols_dfs = [pd.read_csv(tc, index_col=0) for tc in ga_path.glob(\"*.csv\")]\n",
    "trashed_cols_names = [set(tcd.index) for tcd in trashed_cols_dfs]\n",
    "\n",
    "all_trashed_cols = set()\n",
    "for nrc in trashed_cols_names:\n",
    "    all_trashed_cols = all_trashed_cols.union(nrc)\n",
    "\n",
    "diff_trashed_cols = set()\n",
    "for nrc in trashed_cols_names:\n",
    "    diff_trashed_cols = diff_trashed_cols.union(all_trashed_cols.difference(nrc))\n",
    "print(f\"Columns: {diff_trashed_cols} are not removed from all snapshots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all dataframes and check values from removed columns\n",
    "merged_trashed_cols = pd.DataFrame()\n",
    "for idx, tcd in enumerate(trashed_cols_dfs):\n",
    "    merged_trashed_cols = pd.concat([merged_trashed_cols, tcd.T], ignore_index=True)\n",
    "\n",
    "# sanity check\n",
    "assert len(merged_trashed_cols.columns) == len(all_trashed_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dropped_unnecessarly = []\n",
    "for col in merged_trashed_cols.columns:\n",
    "    unique_vals = merged_trashed_cols[col].unique()\n",
    "    if len(unique_vals) != 1:\n",
    "        columns_dropped_unnecessarly.append(col)\n",
    "\n",
    "columns_dropped_unnecessarly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
