{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline for CSV data\n",
    "The aim of this notebook is to parse CSV file to produce a two layer (\"ego\" and \"course\") network that\n",
    "consitsts of three snapshots (1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we import improved raw dataset after rebuttal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"networks\")\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "df_path = \"data/Amman_data+202+FLcumulativew-oAR.csv\"\n",
    "\n",
    "with open(df_path) as f:\n",
    "    _cols = f.readline()[1:-1]\n",
    "_cols = _cols.split(';')\n",
    "dtype_mapper = {c: str for c in _cols if (\"net@\" in c and \"choice\" not in c)}\n",
    "\n",
    "df = pd.read_csv(df_path, converters = dtype_mapper, sep=';')\n",
    "df = df.drop(df.loc[~df[\"group_questionnaire_order\"].isin([1, 2, 3])].index)\n",
    "df = df.set_index(\"metric_id\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is how we used to import original dataset in the first version of manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"networks\")\n",
    "out_path.mkdir(exist_ok=True, parents=True)\n",
    "df_path = \"data/whitby_03_07_Peer Interaction in SA_Jordan_22_Questionnaire 1_2_3_total data set.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(df_path) as f:\n",
    "    _cols = f.readline()[1:-2]\n",
    "_cols = _cols.split('\",\"')\n",
    "dtype_mapper = {c: str for c in _cols if (\"net@\" in c and \"choice\" not in c)}\n",
    "\n",
    "df = pd.read_csv(df_path, converters = dtype_mapper)\n",
    "df = df.drop(df.loc[~df[\"group_questionnaire_order\"].isin([1, 2, 3])].index)\n",
    "df = df.set_index(\"metric_id\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following columns are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_ego_cols = {col for col in df.columns if (\"net\" in col and \"choice\" in col)}\n",
    "edge_course_cols = {col for col in df.columns if \"net\" in col}.difference(edge_ego_cols)\n",
    "node_cols = set(df.columns).difference(edge_course_cols).difference(edge_ego_cols)\n",
    "\n",
    "assert len(edge_ego_cols) + len(edge_course_cols) + len(node_cols) == len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_ego_df = df[[*edge_ego_cols, \"group_questionnaire_order\"]]\n",
    "edge_course_df = df[[*edge_course_cols, \"group_questionnaire_order\"]]\n",
    "node_df = df[list(node_cols)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain weighted adjacency matrix for 'ego' network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_ego_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_snapshot = 3  # <- change this to obtain particular snapshot!\n",
    "_edge_ego_df = edge_ego_df.loc[edge_ego_df[\"group_questionnaire_order\"] == ego_snapshot]\n",
    "_edge_ego_df = _edge_ego_df.drop(\"group_questionnaire_order\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ego_edge_list = []\n",
    "\n",
    "for node, row in _edge_ego_df.iterrows():\n",
    "    for choice_level, choice_node in row.items():\n",
    "        try:\n",
    "            weight = int(choice_level[4])  # e.g. net@2ndchoice -> 2\n",
    "            choice_node = int(choice_node)\n",
    "            ego_edge_list.append({\"source\": node, \"target\": choice_node, \"weight\": weight})\n",
    "        except Exception as e:\n",
    "            print(node, choice_node, choice_level, e)\n",
    "\n",
    "ego_edge_df = pd.DataFrame(ego_edge_list)\n",
    "ego_edge_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_weight = ego_edge_df[\"weight\"].max()\n",
    "ego_edge_df[\"weight\"] =  max_weight - ego_edge_df[\"weight\"] + 1\n",
    "ego_edge_df[\"weight\"] =  ego_edge_df[\"weight\"] / max_weight\n",
    "ego_edge_df[\"weight\"].unique()\n",
    "# 1, 2, 3, 4, 5\n",
    "# 5, 4, 3, 2, 1 \n",
    "# 1, 0.8, 0.6, 0.4, 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_path = out_path / \"ego_edges\"\n",
    "ego_edge_df.to_csv(_out_path.joinpath(f\"{ego_snapshot}_ego_edges.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain weighted edge list for course network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_edge(code: str) -> dict:\n",
    "\n",
    "    if len(code) != 3:\n",
    "        print(code)\n",
    "        return {}\n",
    "    try:\n",
    "        int(code)\n",
    "    except:\n",
    "        print(code)\n",
    "        return {}\n",
    "    \n",
    "    if code[0] == \"1\":\n",
    "        direction = \"out\"\n",
    "    elif code[0] == \"2\":\n",
    "        direction = \"in\"\n",
    "    elif code[0] == \"3\":\n",
    "        direction = \"mutual\"\n",
    "    else:\n",
    "        print(code)\n",
    "        return {}\n",
    "    intensity = int(code[1])  # not yet normalised! \n",
    "    lang_usage = int(code[2]) # not yet normalised!\n",
    "\n",
    "    return {\"direction\": direction, \"intensity\": intensity, \"lang_usage\": lang_usage}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_snapshot = 3  # <- change this to obtain particular snapshot!\n",
    "_edge_course_df = edge_course_df.loc[edge_course_df[\"group_questionnaire_order\"] == course_snapshot]\n",
    "_edge_course_df = _edge_course_df.drop(\"group_questionnaire_order\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_edge_list = []\n",
    "\n",
    "for source_node, row in _edge_course_df.iterrows():\n",
    "    for target_node, code in row.items():\n",
    "        decoded_edge = decode_edge(str(code))\n",
    "        if len(decoded_edge) == 0:\n",
    "            continue\n",
    "        course_edge_list.append({\"source\": source_node, \"target\": int(target_node[4:]), **decoded_edge})\n",
    "\n",
    "course_edge_df = pd.DataFrame(course_edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debugging - sanity chech whether decoded edges are in assumed ranges\n",
    "sns.histplot(course_edge_df[\"intensity\"])  # should be [0, 5]\n",
    "sns.mpl.pyplot.show()\n",
    "sns.histplot(course_edge_df[\"lang_usage\"])  # should be [0, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise intensity and lang_usage\n",
    "course_edge_df[\"intensity\"] = course_edge_df[\"intensity\"].map(lambda x: max(0, min(5, x)))\n",
    "sns.histplot(course_edge_df[\"intensity\"])\n",
    "sns.mpl.pyplot.show()\n",
    "\n",
    "course_edge_df[\"lang_usage\"] = course_edge_df[\"lang_usage\"].map(lambda x: max(0, min(9, x)))\n",
    "sns.histplot(course_edge_df[\"lang_usage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_out_path = out_path / \"course_edges\"\n",
    "course_edge_df.to_csv(_out_path.joinpath(f\"{course_snapshot}_course_edges.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain node list\n",
    "\n",
    "We process here all snapshots at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df[\"group_questionnaire_order\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that don't add any information, i.e. each node has the same value for these columns\n",
    "garbage_attrs = []\n",
    "for col in node_df.columns:\n",
    "    unique_vals = node_df[col].unique()\n",
    "    if len(unique_vals) == 1:\n",
    "        garbage_attrs.append(col)\n",
    "garbage_attrs.sort()\n",
    "\n",
    "# save names of renamed columns for further sainty check\n",
    "_ = {ga: str(node_df[ga].unique()[0]) for ga in garbage_attrs}\n",
    "pd.DataFrame({\"unique_val\": _}).to_csv(out_path / f\"removed_columns.csv\")\n",
    "\n",
    "print(f\"removing {len(garbage_attrs)} columns\")\n",
    "node_df = node_df.drop(garbage_attrs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df_1 = node_df.loc[node_df[\"group_questionnaire_order\"] == 1]\n",
    "node_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df_2 = node_df.loc[node_df[\"group_questionnaire_order\"] == 2]\n",
    "node_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df_3 = node_df.loc[node_df[\"group_questionnaire_order\"] == 3]\n",
    "node_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def obtain_epmty_columns(df: pd.DataFrame) -> List[str]:\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        unique_vals = df[col].unique()\n",
    "        if len(unique_vals) == 1 and pd.isna(unique_vals[0]):\n",
    "            cols_to_drop.append(col)\n",
    "    return cols_to_drop\n",
    "\n",
    "\n",
    "def replace_nans(node_df_x: pd.DataFrame, node_df_reference: pd.DataFrame, idx_col: str):\n",
    "    cols_with_nans = obtain_epmty_columns(node_df_x)\n",
    "    _node_df_x  = node_df_x.drop(cols_with_nans, axis=1)\n",
    "    _node_df_reference = node_df_reference[cols_with_nans]\n",
    "    return _node_df_x.join(_node_df_reference, on=idx_col, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df_2 = replace_nans(node_df_2, node_df_1, \"metric_id\")\n",
    "node_df_2 = node_df_2.sort_index(axis=1).reset_index().rename({\"metric_id\": \"node_id\"}, axis=1)\n",
    "node_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df_3 = replace_nans(node_df_3, node_df_1, \"metric_id\")\n",
    "node_df_3 = node_df_3.sort_index(axis=1).reset_index().rename({\"metric_id\": \"node_id\"}, axis=1)\n",
    "node_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df_1 = node_df_1.sort_index(axis=1).reset_index().rename({\"metric_id\": \"node_id\"}, axis=1)\n",
    "node_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_df_1.to_csv(out_path.joinpath(\"1_nodes.csv\"))\n",
    "node_df_2.to_csv(out_path.joinpath(\"2_nodes.csv\"))\n",
    "node_df_3.to_csv(out_path.joinpath(\"3_nodes.csv\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhance node list by additional data (housing and their pre-exam results)\n",
    "\n",
    "This is done for each snaphsot separately (not obligatory for raw data after rebuttal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = pd.read_csv(\n",
    "    \"data/2022 Issues Classes and Final Exam scores(80).csv\",\n",
    "    sep=\";\",\n",
    "    decimal=\",\",\n",
    ")\n",
    "print(len(final_scores))\n",
    "final_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dfs = [(\"1_nodes.csv\", node_df_1), (\"2_nodes.csv\", node_df_2), (\"3_nodes.csv\", node_df_3)]\n",
    "out_name, node_df_x = node_dfs[2]  # <- select this manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nodes = node_df_x.merge(\n",
    "    final_scores, how=\"left\", left_on=\"metric_Name\", right_on=\"student_name\"\n",
    ").drop(\"student_name\", axis=1)\n",
    "\n",
    "print(len(merged_nodes))\n",
    "merged_nodes.head()\n",
    "\n",
    "# merged_nodes.to_csv(out_path.joinpath(out_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3821d06bbeef519991c7543666dd5715403d3309e438c14e45f72ffb3c1c738f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
